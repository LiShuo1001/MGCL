import os
import torch
import random
import logging
import datetime
import numpy as np
import torch.optim as optim
from alisuretool.Tools import Tools
from tensorboardX import SummaryWriter


class MyCommon(object):

    @staticmethod
    def fix_randseed(seed):
        r""" Set random seeds for reproducibility """
        if seed is None:
            seed = int(random.random() * 1e5)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True

    @staticmethod
    def mean(x):
        return sum(x) / len(x) if len(x) > 0 else 0.0

    @staticmethod
    def to_cuda(batch, device):
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                batch[key] = value.to(device)
        return batch

    @staticmethod
    def to_cpu(tensor):
        return tensor.detach().clone().cpu()

    @staticmethod
    def gpu_setup(use_gpu, gpu_id):
        if torch.cuda.is_available() and use_gpu:
            Tools.print()
            Tools.print('Cuda available with GPU: {}'.format(torch.cuda.get_device_name(0)))
            os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
            os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
            device = torch.device("cuda:{}".format(gpu_id))
        else:
            Tools.print()
            Tools.print('Cuda not available')
            device = torch.device("cpu")
        return device

    @staticmethod
    def mask_to_rle_pytorch(tensor):
        # Put in fortran order and flatten h,w
        b, h, w = tensor.shape
        tensor = tensor.permute(0, 2, 1).flatten(1)

        # Compute change indices
        diff = tensor[:, 1:] ^ tensor[:, :-1]
        change_indices = diff.nonzero()

        # Encode run length
        out = []
        for i in range(b):
            cur_idxs = change_indices[change_indices[:, 0] == i, 1]
            cur_idxs = torch.cat(
                [
                    torch.tensor([0], dtype=cur_idxs.dtype, device=cur_idxs.device),
                    cur_idxs + 1,
                    torch.tensor([h * w], dtype=cur_idxs.dtype, device=cur_idxs.device),
                ]
            )
            btw_idxs = cur_idxs[1:] - cur_idxs[:-1]
            counts = [] if tensor[i, 0] == 0 else [0]
            counts.extend(btw_idxs.detach().cpu().tolist())
            out.append({"size": [h, w], "counts": counts})
        return out

    @staticmethod
    def rle_to_mask(rle):
        """Compute a binary mask from an uncompressed RLE."""
        h, w = rle["size"]
        mask = np.empty(h * w, dtype=bool)
        idx = 0
        parity = False
        for count in rle["counts"]:
            mask[idx: idx + count] = parity
            idx += count
            parity ^= True
        mask = mask.reshape(w, h)
        return mask.transpose()  # Put in C order

    pass


lr_ratio_list = [1000, 500, 1, 2]


class MyOptim(object):

    @classmethod
    def get_finetune_optimizer(cls, args, model):
        if args.finetune_backbone:
            return cls.get_finetune_optimizer_backbone(args, model)
        else:
            return cls.get_finetune_optimizer_no_backbone(args, model)
        pass

    @classmethod
    def adjust_learning_rate_poly(cls, args, optimizer, iter, max_iter):
        if args.finetune_backbone:
            return cls.adjust_learning_rate_poly_backbone(args, optimizer, iter, max_iter)
        else:
            return cls.adjust_learning_rate_poly_no_backbone(args, optimizer, iter, max_iter)
        pass

    @staticmethod
    def get_finetune_optimizer_no_backbone(args, model):
        lr = args.lr
        weight_list = []
        bias_list = []
        pretrain_weight_list = []
        pretrain_bias_list = []
        for name, value in model.named_parameters():
            if 'model_res' in name or 'model_backbone' in name or 'backbone' in name:
                if 'weight' in name:
                    pretrain_weight_list.append(value)
                elif 'bias' in name:
                    pretrain_bias_list.append(value)
            else:
                if 'weight' in name:
                    weight_list.append(value)
                elif 'bias' in name:
                    bias_list.append(value)

        if args.is_sgd:
            opt = optim.SGD([{'params': weight_list, 'lr': lr * lr_ratio_list[2]},
                             {'params': bias_list, 'lr': lr * lr_ratio_list[3]}], momentum=0.90, weight_decay=0.0005)
        else:
            opt = optim.Adam([{'params': weight_list, 'lr': lr * lr_ratio_list[2]},
                              {'params': bias_list, 'lr': lr * lr_ratio_list[3]}], weight_decay=0.0005)
        return opt

    @staticmethod
    def adjust_learning_rate_poly_no_backbone(args, optimizer, iter, max_iter):
        lr = args.lr * ((1 - float(iter) / max_iter) ** args.power)
        optimizer.param_groups[0]['lr'] = lr * lr_ratio_list[2]
        optimizer.param_groups[1]['lr'] = lr * lr_ratio_list[3]
        return lr

    @staticmethod
    def get_finetune_optimizer_backbone(args, model):
        lr = args.lr
        weight_list = []
        bias_list = []
        pretrain_weight_list = []
        pretrain_bias_list = []
        for name, value in model.named_parameters():
            if 'model_res' in name or 'model_backbone' in name or 'backbone' in name:
                if 'weight' in name:
                    pretrain_weight_list.append(value)
                elif 'bias' in name:
                    pretrain_bias_list.append(value)
            else:
                if 'weight' in name:
                    weight_list.append(value)
                elif 'bias' in name:
                    bias_list.append(value)

        if args.is_sgd:
            opt = optim.SGD([{'params': pretrain_weight_list, 'lr': lr / lr_ratio_list[0]},
                             {'params': pretrain_bias_list, 'lr': lr / lr_ratio_list[1]},
                             {'params': weight_list, 'lr': lr * lr_ratio_list[2]},
                             {'params': bias_list, 'lr': lr * lr_ratio_list[3]}], momentum=0.90, weight_decay=0.0005)
        else:
            opt = optim.Adam([{'params': pretrain_weight_list, 'lr': lr / lr_ratio_list[0]},
                              {'params': pretrain_bias_list, 'lr': lr / lr_ratio_list[1]},
                              {'params': weight_list, 'lr': lr * lr_ratio_list[2]},
                              {'params': bias_list, 'lr': lr * lr_ratio_list[3]}], weight_decay=0.0005)
        return opt

    @staticmethod
    def adjust_learning_rate_poly_backbone(args, optimizer, iter, max_iter):
        lr = args.lr * ((1 - float(iter) / max_iter) ** args.power)
        optimizer.param_groups[0]['lr'] = lr / lr_ratio_list[0]
        optimizer.param_groups[1]['lr'] = lr / lr_ratio_list[1]
        optimizer.param_groups[2]['lr'] = lr * lr_ratio_list[2]
        optimizer.param_groups[3]['lr'] = lr * lr_ratio_list[3]
        return lr

    pass


class AverageMeter:
    r""" Stores loss, evaluation results """
    def __init__(self, dataset, device=None):
        self.benchmark = dataset.benchmark
        self.class_ids_interest = dataset.class_ids
        self.class_ids_interest = torch.tensor(self.class_ids_interest).to(device)

        if self.benchmark == 'isaid':
            self.nclass = 15
        elif self.benchmark == 'dlrsd':
            self.nclass = 15
        elif self.benchmark == 'pascal':
            self.nclass = 20
        elif self.benchmark == 'coco':
            self.nclass = 80
        elif self.benchmark == 'fss':
            self.nclass = 1000

        self.intersection_buf = torch.zeros([2, self.nclass]).float().to(device)
        self.union_buf = torch.zeros([2, self.nclass]).float().to(device)
        self.ones = torch.ones_like(self.union_buf)
        self.loss_buf = []

    def update(self, inter_b, union_b, class_id, loss):
        self.intersection_buf.index_add_(1, class_id, inter_b.float())
        self.union_buf.index_add_(1, class_id, union_b.float())
        if loss is None:
            loss = torch.tensor(0.0)
        self.loss_buf.append(loss)

    def compute_iou(self):
        iou = self.intersection_buf.float() / \
              torch.max(torch.stack([self.union_buf, self.ones]), dim=0)[0]
        iou = iou.index_select(1, self.class_ids_interest)
        miou = iou[1].mean() * 100

        fb_iou = (self.intersection_buf.index_select(1, self.class_ids_interest).sum(dim=1) /
                  self.union_buf.index_select(1, self.class_ids_interest).sum(dim=1)).mean() * 100

        return miou, fb_iou

    def compute_iou_class(self):
        iou = self.intersection_buf.float() / \
              torch.max(torch.stack([self.union_buf, self.ones]), dim=0)[0]
        iou = iou.index_select(1, self.class_ids_interest)
        miou = iou[1].mean() * 100

        fb_iou = (self.intersection_buf.index_select(1, self.class_ids_interest).sum(dim=1) /
                  self.union_buf.index_select(1, self.class_ids_interest).sum(dim=1)).mean() * 100

        return miou, fb_iou, iou[1]

    def write_result(self, split, epoch):
        iou, fb_iou = self.compute_iou()

        loss_buf = torch.stack(self.loss_buf)
        msg = '\n*** %s ' % split
        msg += '[@Epoch %02d] ' % epoch
        msg += 'Avg L: %6.5f  ' % loss_buf.mean()
        msg += 'mIoU: %5.2f   ' % iou
        msg += 'FB-IoU: %5.2f   ' % fb_iou

        msg += '***\n'
        Logger.info(msg)

    def write_process(self, batch_idx, datalen, epoch, write_batch_idx=20):
        if batch_idx % write_batch_idx == 0:
            msg = '[Epoch: %02d] ' % epoch if epoch != -1 else ''
            msg += '[Batch: %04d/%04d] ' % (batch_idx+1, datalen)
            iou, fb_iou = self.compute_iou()
            if epoch != -1:
                loss_buf = torch.stack(self.loss_buf)
                msg += 'L: %6.5f  ' % loss_buf[-1]
                msg += 'Avg L: %6.5f  ' % loss_buf.mean()
            msg += 'mIoU: %5.2f  |  ' % iou
            msg += 'FB-IoU: %5.2f' % fb_iou
            Logger.info(msg)
            pass
        pass
    pass


class Logger:
    r""" Writes evaluation results of training/testing """
    @classmethod
    def initialize(cls, args, training):
        logtime = datetime.datetime.now().__format__('_%m%d_%H%M%S')
        logpath = args.logpath if training else '_TEST_' + args.load.split('/')[-2].split('.')[0] + logtime
        if logpath == '': logpath = logtime

        cls.logpath = os.path.join('logs', logpath + '.log')
        cls.benchmark = args.benchmark
        if not os.path.exists(cls.logpath):
            os.makedirs(cls.logpath)

        logging.basicConfig(filemode='w',
                            filename=os.path.join(cls.logpath, 'log.txt'),
                            level=logging.INFO,
                            format='%(message)s',
                            datefmt='%m-%d %H:%M:%S')

        # Console log config
        console = logging.StreamHandler()
        console.setLevel(logging.INFO)
        formatter = logging.Formatter('%(message)s')
        console.setFormatter(formatter)
        logging.getLogger('').addHandler(console)

        # Tensorboard writer
        cls.tbd_writer = SummaryWriter(os.path.join(cls.logpath, 'tbd/runs'))

        # Log arguments
        logging.info('\n:=========== Few-shot Seg. with HSNet ===========')
        for arg_key in args.__dict__:
            logging.info('| %20s: %-24s' % (arg_key, str(args.__dict__[arg_key])))
        logging.info(':================================================\n')

    @classmethod
    def info(cls, msg):
        r""" Writes log message to log.txt """
        logging.info(msg)

    @classmethod
    def save_model_miou(cls, model, epoch, val_miou):
        torch.save(model.state_dict(), os.path.join(cls.logpath, 'best_model.pt'))
        cls.info('Model saved @%d w/ val. mIoU: %5.2f.\n' % (epoch, val_miou))

    @classmethod
    def log_params(cls, model):
        backbone_param = 0
        learner_param = 0
        for k in model.state_dict().keys():
            n_param = model.state_dict()[k].view(-1).size(0)
            if k.split('.')[0] in 'backbone':
                if k.split('.')[1] in ['classifier', 'fc']:  # as fc layers are not used in HSNet
                    continue
                backbone_param += n_param
            else:
                learner_param += n_param
        Logger.info('Backbone # param.: %d' % backbone_param)
        Logger.info('Learnable # param.: %d' % learner_param)
        Logger.info('Total # param.: %d' % (backbone_param + learner_param))

    pass
